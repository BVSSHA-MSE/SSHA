# [Coherence] - SSH Advocacy Embedded Portfolio-Wide

**Year:** 2025  
**Level:** Portfolio  
**Theme:** Coherence  
**Status:** Achieved (2025-12-25)

## Context
Currently, we are not performing from outcomes but from activity, which prevents us from practicing what we are advocating and compromises our integrity.

## Desired Outcome

### Better
Portfolio decisions and investment choices explicitly framed in outcomes-with-why; decision-makers can state desired change and rationale in under 2 minutes.

### Value
Every portfolio item tied to 1–2 clear outcomes; activity-only proposals rejected at intake.

### Sooner
Portfolio proposals surface outcome gaps at intake, not mid-execution; faster investment decisions.

### Safer
Integrity risk eliminated—external advocacy matches internal practice; zero "we say outcomes, do activities" exceptions in portfolio governance.

### Happier
Decision-makers have clarity on strategic intent; reduced cognitive load from aligning advocacy messaging with actual practice.

**Primary dimensions:** Safer (integrity/credibility), Value (strategic alignment over activity)
**Supporting:** Better (decision quality), Sooner (faster investment cycles), Happier (strategic clarity)

**Enabled by:**
- [Program: Easier to Drive Change with BVSSH Outcomes](../program/easier-to-drive-change-with-bvssh-outcomes.md)
- [Team: Outcome-First Becomes Default Through Automation](../team/outcome-first-default-through-automation.md)
- [Team: Outcome Fluency Through Mapping](../team/outcome-fluency-through-mapping.md)

## Hypothesis
We believe that requiring outcome-first framing in all portfolio proposals and governing through a BVSSH lens (enabled by Program adoption ease and Team automation),
Will result in eliminated integrity risk and improved investment decision quality—zero activity-only approvals, advocacy messaging matches practice.
We believe this because when the Program makes outcomes easy and the Team automates quality, portfolio governance can enforce outcome-first without friction, eliminating the "we say outcomes, do activities" contradiction.

## Expected Behavioral Change

### Before
Approving portfolio items based on activity descriptions; advocacy messaging disconnected from internal practice.

### Stop
- Approving proposals that lack outcome articulation
- Accepting "improve X" as sufficient outcome framing
- External advocacy claims that contradict internal portfolio decisions

### Start
- Rejecting activity-only proposals at intake (before investment)
- Opening portfolio reviews with "what outcome does this enable?" as first question
- Aligning external advocacy messages with portfolio governance practices

### After
Portfolio decisions require outcome articulation; investment approvals explicitly reference outcomes; advocacy messaging reflects our practice.

## Signals & Measures

### Leading
- Median specificity score of outcome statements (e.g., 1–5 rubric: 1=vague aspiration, 5=clear hypothesis + primary dimension)
- % of proposals where hypothesis connects action to expected behavioral change (avoid circular logic)
- % of portfolio proposals that state desired behavioral change before listing activities (trend: increase)

### Lagging
- % of active portfolio items with articulated outcome + aligned behaviors in practice (trend up = integrity improving)
- % of active portfolio items where we can articulate both what outcome we're pursuing AND why in <2 minutes (trend: increase)
- Instances of mid-execution strategy/activity pivots due to outcome misalignment (trend: decrease)

## Constraints & Guardrails

- **Time/income link unclear:** Trade-off - focus portfolio on high-impact work; accept slower expansion.
- **Passion can put people off:** Trade-off - balance advocacy intensity with listening and pragmatism.
- **AI dependency + own thinking:** Trade-off - use AI for speed/drafts, but own validation and reasoning.

## Review Cadence
Quarterly review with AI partner incorporates learning from active Program outcome reviews (e.g., adoption metrics, friction points, hypothesis adjustments).

### Decision points:

- **Continue:** Leading and lagging signals trending in expected direction (or holding steady).
- **Adjust hypothesis:** Signals flat/mixed; hypothesis may need refinement or behavioral changes need reset.
- **Pivot:** Signals trending opposite to expectation; underlying assumption invalidated; new approach needed.

## Reflection (Post-Outcome)

### What Actually Happened vs. Hypothesis
**Hypothesis proved correct—with acceleration:** Outcome-first governance enabled by Program + Team automation created integrity alignment and strategic clarity. All dependencies completed; portfolio-wide practice shift achieved.

**Evidence:**
- ✅ Both Team outcomes completed (automation + fluency fully operationalized)
- ✅ Program outcome completed (adoption friction removed; BVSSH habit formed)
- ✅ 100% of active portfolio items articulated with clear outcomes + behavioral change + measurable trends
- ✅ Median outcome specificity score >4.5/5 (clear hypothesis, primary dimension, rationale)
- ✅ 100% of outcomes outcome-first (no activity-only approvals)
- ✅ Zero mid-execution pivots due to outcome misalignment
- ✅ Outcomes clear enough for verbal repetition without notes
- ✅ Quality automation embedded in outcomes (catches shallow outcomes pre-approval)
- ✅ Claims match decisions (advocacy messaging aligned with portfolio practice based on current understanding of message)
- ✅ Learning captured throughout kb/ and within outcome files

### Validated Assumptions
✅ **Integrity risk was primarily a governance gap:** When intake gates enforce outcome-first and quality automation validates specificity, activity-only proposals never reach approval. Zero exceptions.

✅ **Automation enables integrity at scale:** Team automation (templating, quality checks) made outcome-first governance frictionless; no "we can't enforce this without overhead" excuse held.

✅ **Clarity drives decision speed:** Proposals with articulated outcomes, behavioral change, and measured trends decided faster and with higher confidence.

✅ **Practice-messaging coherence is achievable:** Intentional alignment checks (throughout kb/) suggest that external SSH advocacy claims trace to active portfolio outcomes;

### Key Learnings
1. **Portfolio integrity is a system property:** Not achieved by willpower alone. Requires (1) Program ease, (2) Team automation, (3) Portfolio governance gates. All three required.
2. **Outcome clarity compounds:** When 100% of items have clear outcomes, decision-makers develop intuition; verbal articulation becomes natural (Happier).
3. **Quality automation prevents drift:** Specificity scores >4.5/5 weren't achieved through discipline alone—automation flagged vague/circular outcomes pre-approval.
4. **Learning layers matter:** Documented kb/ sessions + within-outcome reflections captured hypothesis evolution; inform 2026 candidate outcomes.

### Invalidated/Modified Assumptions
⚠️ **Intake gate complexity:** Initially feared governance gates would add friction. In practice, automated checklists (Team automation) eliminated friction; gates became routine.

✅ **Behavioral change measurement:** Measurement enabled by Team automation; no separate process needed.

### What to Change Next Time (2026+)
- **Scaling depth:** 100% coverage at >4.5/5 is healthy baseline. 2026 focus: deeper behavioral measurement (are portfolio decisions actually producing claimed outcomes in practice?).
- **Advocacy audit cadence:** Alignment checks happened "as we understand"; formalize quarterly advocacy-to-portfolio audit.
- **Learnings persistence:** kb/ logs are valuable; 2026 should define what gets archived/indexed for portfolio governance reference (vs. session notes).
- **Dependency clarity:** This outcome's success was 100% dependent on Program + Team completions. Document dependency chains explicitly in 2026 outcomes.

---

## 2026 Candidate Outcomes (From Dependencies + Learnings)

**From Program outcome (completed):**
- Outcome status tracking (missed in 2025; critical for 2026 automation)
- Monthly spot checks formalization (commit message sampling, quality review cadence)
- Portfolio-level reporting automation

**From this Portfolio outcome (completed):**
- **Advocacy Alignment Audit (formalize quarterly checks):** Ensure external claims remain traceable to portfolio decisions; build into governance cadence
- **Portfolio Outcome Performance Tracking:** Measure if portfolio outcomes are actually producing claimed behavioral change; move from "outputs aligned" to "outcomes validated"
- **Portfolio Dependency Mapping:** Visualize how Portfolio ← Program ← Team outcomes chain together; enables clearer planning